"""
ğŸš€ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ìµœì í™” ì‹œìŠ¤í…œ - ì¸ë±ì‹±, ë°°ì¹˜ ì²˜ë¦¬, ë³‘ë ¬ ê²€ìƒ‰
"""

import asyncio
import time
import threading
from typing import List, Dict, Any, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import numpy as np
from datetime import datetime, timedelta
import json

# Pinecone ë° OpenAI ì—°ê²°
from scripts.connection_manager import connection_manager
from scripts.filtered_vector_search import FilteredVectorSearch

@dataclass
class SearchMetrics:
    """ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    total_searches: int = 0
    avg_search_time: float = 0.0
    cache_hits: int = 0
    vector_hits: int = 0
    errors: int = 0
    last_optimization: Optional[datetime] = None

class OptimizedVectorSearchManager:
    """
    ğŸš€ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ê´€ë¦¬ì
    
    ì£¼ìš” ìµœì í™”:
    - ë°°ì¹˜ ê²€ìƒ‰
    - ê²°ê³¼ ìºì‹±
    - ë³‘ë ¬ ì²˜ë¦¬
    - ì¸ë±ìŠ¤ ìµœì í™”
    - ì§€ëŠ¥í˜• í•„í„°ë§
    """
    
    def __init__(self, 
                 batch_size: int = 10,
                 max_workers: int = 4,
                 cache_ttl_minutes: int = 30):
        
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.cache_ttl = timedelta(minutes=cache_ttl_minutes)
        
        # ê¸°ë³¸ ê²€ìƒ‰ê¸°
        self.base_searcher: Optional[FilteredVectorSearch] = None
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.search_cache: Dict[str, Tuple[List[Tuple[dict, float]], datetime]] = {}
        self.cache_lock = threading.RLock()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = SearchMetrics()
        
        # ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # ìì£¼ ê²€ìƒ‰ë˜ëŠ” íŒ¨í„´ ì¶”ì 
        self.query_patterns: Dict[str, int] = {}
        self.pattern_lock = threading.RLock()
        
        # ìµœì í™” ì„¤ì •
        self.enable_parallel_search = True
        self.enable_smart_filtering = True
        self.enable_result_prefetching = True
    
    def initialize(self):
        """ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.base_searcher = connection_manager.vector_searcher
            print("âœ… ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _generate_search_cache_key(self, query: str, category: Optional[str], 
                                  section: Optional[str], k: int, 
                                  score_threshold: float) -> str:
        """ê²€ìƒ‰ ìºì‹œ í‚¤ ìƒì„±"""
        key_data = {
            'query': query.lower().strip(),
            'category': category,
            'section': section,
            'k': k,
            'threshold': score_threshold
        }
        return json.dumps(key_data, sort_keys=True)
    
    def _is_cache_valid(self, timestamp: datetime) -> bool:
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        return datetime.now() - timestamp < self.cache_ttl
    
    def _get_from_cache(self, cache_key: str) -> Optional[List[Tuple[dict, float]]]:
        """ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ"""
        with self.cache_lock:
            if cache_key in self.search_cache:
                results, timestamp = self.search_cache[cache_key]
                if self._is_cache_valid(timestamp):
                    self.metrics.cache_hits += 1
                    return results
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self.search_cache[cache_key]
        return None
    
    def _store_in_cache(self, cache_key: str, results: List[Tuple[dict, float]]):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        with self.cache_lock:
            self.search_cache[cache_key] = (results, datetime.now())
            
            # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ)
            if len(self.search_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = min(self.search_cache.keys(), 
                               key=lambda k: self.search_cache[k][1])
                del self.search_cache[oldest_key]
    
    def _track_query_pattern(self, query: str):
        """ìì£¼ ê²€ìƒ‰ë˜ëŠ” íŒ¨í„´ ì¶”ì """
        with self.pattern_lock:
            # ì¿¼ë¦¬ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ íŒ¨í„´ ì¶”ì¶œ
            pattern = " ".join(sorted(query.lower().split()))
            self.query_patterns[pattern] = self.query_patterns.get(pattern, 0) + 1
    
    def _apply_smart_filtering(self, query: str, category: Optional[str], 
                             section: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
        """ì§€ëŠ¥í˜• í•„í„°ë§ - ì¿¼ë¦¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìµœì  í•„í„° ì¶”ì²œ"""
        if not self.enable_smart_filtering:
            return category, section
        
        query_lower = query.lower()
        
        # ì¹´í…Œê³ ë¦¬ ìë™ ì¶”ë¡ 
        if not category:
            if any(word in query_lower for word in ['ì²´í¬ì¸', 'ì…ì‹¤', 'ë„ì°©']):
                category = 'checkin'
            elif any(word in query_lower for word in ['ì²´í¬ì•„ì›ƒ', 'í‡´ì‹¤', 'ì¶œë°œ']):
                category = 'checkout'
            elif any(word in query_lower for word in ['ì‹ë‹¹', 'ë ˆìŠ¤í† ë‘', 'ë·”í˜', 'ì¡°ì‹']):
                category = 'dining'
            elif any(word in query_lower for word in ['ìˆ˜ì˜ì¥', 'í—¬ìŠ¤ì¥', 'ìŠ¤íŒŒ', 'ì‹œì„¤']):
                category = 'facilities'
        
        return category, section
    
    async def similarity_search_async(self, 
                                    query: str,
                                    k: int = 3,
                                    category: Optional[str] = None,
                                    section: Optional[str] = None,
                                    score_threshold: float = 0.7) -> List[Tuple[dict, float]]:
        """
        ğŸš€ ë¹„ë™ê¸° ìµœì í™”ëœ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Returns:
            List[Tuple[dict, float]]: (ë©”íƒ€ë°ì´í„°, ì ìˆ˜) ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        self.metrics.total_searches += 1
        
        try:
            # ê²€ìƒ‰ íŒ¨í„´ ì¶”ì 
            self._track_query_pattern(query)
            
            # ì§€ëŠ¥í˜• í•„í„°ë§ ì ìš©
            category, section = self._apply_smart_filtering(query, category, section)
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_search_cache_key(query, category, section, k, score_threshold)
            cached_results = self._get_from_cache(cache_key)
            if cached_results:
                return cached_results
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            if not self.base_searcher:
                self.initialize()
            
            if self.base_searcher:
                results = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.base_searcher.similarity_search_with_metadata,
                    query, k, category, section, score_threshold
                )
                
                # ê²°ê³¼ ìºì‹±
                self._store_in_cache(cache_key, results)
                self.metrics.vector_hits += 1
                
                return results
            else:
                self.metrics.errors += 1
                return []
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            self.metrics.errors += 1
            return []
        
        finally:
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            search_time = time.time() - start_time
            self._update_avg_search_time(search_time)
    
    async def batch_search(self, 
                          queries: List[str],
                          k: int = 3,
                          category: Optional[str] = None,
                          section: Optional[str] = None,
                          score_threshold: float = 0.7) -> List[List[Tuple[dict, float]]]:
        """
        ğŸš€ ë°°ì¹˜ ê²€ìƒ‰ - ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[List[Tuple[dict, float]]]: ê° ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.enable_parallel_search or len(queries) == 1:
            # ìˆœì°¨ ì²˜ë¦¬
            results = []
            for query in queries:
                result = await self.similarity_search_async(
                    query, k, category, section, score_threshold
                )
                results.append(result)
            return results
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = [
            self.similarity_search_async(query, k, category, section, score_threshold)
            for query in queries
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                print(f"âš ï¸ ë°°ì¹˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(result)}")
                processed_results.append([])
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def smart_search_with_expansion(self, 
                                        query: str,
                                        k: int = 3,
                                        category: Optional[str] = None,
                                        section: Optional[str] = None,
                                        score_threshold: float = 0.7,
                                        expand_queries: bool = True) -> List[Tuple[dict, float]]:
        """
        ğŸ§  ì§€ëŠ¥í˜• ê²€ìƒ‰ - ì¿¼ë¦¬ í™•ì¥ ë° ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ ì ìš©
        
        Args:
            expand_queries: ì¿¼ë¦¬ í™•ì¥ ì—¬ë¶€
            
        Returns:
            List[Tuple[dict, float]]: ìµœì í™”ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        queries_to_search = [query]
        
        if expand_queries:
            # ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, ìœ ì‚¬ í‘œí˜„ ì¶”ê°€)
            expanded_queries = self._expand_query(query)
            queries_to_search.extend(expanded_queries)
        
        # ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰
        all_results = await self.batch_search(
            queries_to_search, k, category, section, score_threshold
        )
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        combined_results = self._merge_and_deduplicate_results(all_results)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ë°˜í™˜
        return combined_results[:k]
    
    def _expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥ - ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ ìƒì„±"""
        expanded = []
        query_lower = query.lower()
        
        # í˜¸í…” ê´€ë ¨ ë™ì˜ì–´ ë§¤í•‘
        synonyms_map = {
            'ì²´í¬ì¸': ['ì…ì‹¤', 'ë„ì°©', 'ì…ì‹¤ì‹œê°„'],
            'ì²´í¬ì•„ì›ƒ': ['í‡´ì‹¤', 'ì¶œë°œ', 'í‡´ì‹¤ì‹œê°„'],
            'ì¡°ì‹': ['ì•„ì¹¨ì‹ì‚¬', 'ë¸Œë™í¼ìŠ¤íŠ¸', 'ëª¨ë‹ë¶€í˜'],
            'ìˆ˜ì˜ì¥': ['í’€', 'ìˆ˜ì˜ì‹œì„¤'],
            'ì£¼ì°¨': ['ì£¼ì°¨ì¥', 'íŒŒí‚¹'],
            'ì™€ì´íŒŒì´': ['wifi', 'ì¸í„°ë„·', 'ë¬´ì„ ì¸í„°ë„·'],
        }
        
        for original, synonyms in synonyms_map.items():
            if original in query_lower:
                for synonym in synonyms:
                    expanded_query = query_lower.replace(original, synonym)
                    if expanded_query != query_lower:
                        expanded.append(expanded_query)
        
        return expanded[:2]  # ìµœëŒ€ 2ê°œ í™•ì¥ ì¿¼ë¦¬
    
    def _merge_and_deduplicate_results(self, 
                                     all_results: List[List[Tuple[dict, float]]]) -> List[Tuple[dict, float]]:
        """ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ê³  ì¤‘ë³µ ì œê±°"""
        seen_texts: Set[str] = set()
        merged_results = []
        
        # ëª¨ë“  ê²°ê³¼ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        all_items = []
        for results in all_results:
            all_items.extend(results)
        
        all_items.sort(key=lambda x: x[1], reverse=True)
        
        for metadata, score in all_items:
            text = metadata.get('text', '')
            if text and text not in seen_texts:
                seen_texts.add(text)
                merged_results.append((metadata, score))
        
        return merged_results
    
    def _update_avg_search_time(self, search_time: float):
        """í‰ê·  ê²€ìƒ‰ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        total = self.metrics.total_searches
        current_avg = self.metrics.avg_search_time
        
        if total == 1:
            self.metrics.avg_search_time = search_time
        else:
            # ì§€ìˆ˜ ì´ë™ í‰ê· 
            alpha = 2.0 / (total + 1)
            self.metrics.avg_search_time = alpha * search_time + (1 - alpha) * current_avg
    
    async def optimize_cache(self):
        """ìºì‹œ ìµœì í™” - ë§Œë£Œëœ í•­ëª© ì •ë¦¬ ë° ì„±ëŠ¥ ì¡°ì •"""
        with self.cache_lock:
            # ë§Œë£Œëœ ìºì‹œ í•­ëª© ì œê±°
            current_time = datetime.now()
            expired_keys = [
                key for key, (_, timestamp) in self.search_cache.items()
                if current_time - timestamp >= self.cache_ttl
            ]
            
            for key in expired_keys:
                del self.search_cache[key]
            
            print(f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {len(expired_keys)}ê°œ ë§Œë£Œ í•­ëª© ì œê±°")
        
        self.metrics.last_optimization = datetime.now()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        cache_hit_rate = (
            self.metrics.cache_hits / max(self.metrics.total_searches, 1) * 100
        )
        
        top_patterns = sorted(
            self.query_patterns.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:10]
        
        return {
            'total_searches': self.metrics.total_searches,
            'avg_search_time_ms': round(self.metrics.avg_search_time * 1000, 2),
            'cache_hit_rate': round(cache_hit_rate, 2),
            'cache_size': len(self.search_cache),
            'vector_hits': self.metrics.vector_hits,
            'errors': self.metrics.errors,
            'top_query_patterns': top_patterns,
            'last_optimization': self.metrics.last_optimization.isoformat() if self.metrics.last_optimization else None,
            'configuration': {
                'batch_size': self.batch_size,
                'max_workers': self.max_workers,
                'cache_ttl_minutes': self.cache_ttl.total_seconds() / 60,
                'parallel_search_enabled': self.enable_parallel_search,
                'smart_filtering_enabled': self.enable_smart_filtering
            }
        }
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.executor:
            self.executor.shutdown(wait=True)

# ì „ì—­ ìµœì í™”ëœ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤
_optimized_searcher: Optional[OptimizedVectorSearchManager] = None
_searcher_lock = threading.Lock()

def get_optimized_searcher() -> OptimizedVectorSearchManager:
    """ì „ì—­ ìµœì í™”ëœ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _optimized_searcher
    
    if _optimized_searcher is None:
        with _searcher_lock:
            if _optimized_searcher is None:
                _optimized_searcher = OptimizedVectorSearchManager()
                _optimized_searcher.initialize()
    
    return _optimized_searcher

# í¸ì˜ í•¨ìˆ˜ë“¤
async def optimized_search(query: str, 
                         k: int = 3,
                         category: Optional[str] = None,
                         section: Optional[str] = None,
                         score_threshold: float = 0.7) -> List[Tuple[dict, float]]:
    """ìµœì í™”ëœ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    searcher = get_optimized_searcher()
    return await searcher.similarity_search_async(query, k, category, section, score_threshold)

async def smart_search(query: str, 
                      k: int = 3,
                      category: Optional[str] = None,
                      section: Optional[str] = None,
                      score_threshold: float = 0.7) -> List[Tuple[dict, float]]:
    """ì§€ëŠ¥í˜• ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    searcher = get_optimized_searcher()
    return await searcher.smart_search_with_expansion(query, k, category, section, score_threshold) 